{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2029267,
          "sourceType": "datasetVersion",
          "datasetId": 815421
        }
      ],
      "dockerImageVersionId": 30646,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-02-05T16:43:06.890868Z",
          "iopub.execute_input": "2024-02-05T16:43:06.891587Z",
          "iopub.status.idle": "2024-02-05T16:43:07.725824Z",
          "shell.execute_reply.started": "2024-02-05T16:43:06.891560Z",
          "shell.execute_reply": "2024-02-05T16:43:07.725262Z"
        },
        "trusted": true,
        "id": "rAoVycS0d2E2",
        "outputId": "8d6d6935-5227-4f74-c779-edabae3461bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/indian-music-raga/malkauns04.wav\n/kaggle/input/indian-music-raga/yaman02.wav\n/kaggle/input/indian-music-raga/malkauns26.wav\n/kaggle/input/indian-music-raga/asavari25.wav\n/kaggle/input/indian-music-raga/bageshree04.wav\n/kaggle/input/indian-music-raga/sarang04.wav\n/kaggle/input/indian-music-raga/sarang16.wav\n/kaggle/input/indian-music-raga/bageshree29.wav\n/kaggle/input/indian-music-raga/bhairavi29.wav\n/kaggle/input/indian-music-raga/bhoop03.wav\n/kaggle/input/indian-music-raga/asavari26.wav\n/kaggle/input/indian-music-raga/Bhairavi01.wav\n/kaggle/input/indian-music-raga/yaman01.wav\n/kaggle/input/indian-music-raga/bhairavi30.wav\n/kaggle/input/indian-music-raga/yaman03.wav\n/kaggle/input/indian-music-raga/bhoop02.wav\n/kaggle/input/indian-music-raga/yaman27.wav\n/kaggle/input/indian-music-raga/bhairavi27.wav\n/kaggle/input/indian-music-raga/sarang05.wav\n/kaggle/input/indian-music-raga/yaman24.wav\n/kaggle/input/indian-music-raga/bhoopali24.wav\n/kaggle/input/indian-music-raga/DKanada01.wav\n/kaggle/input/indian-music-raga/bhoop04.wav\n/kaggle/input/indian-music-raga/bageshree27.wav\n/kaggle/input/indian-music-raga/asavari29.wav\n/kaggle/input/indian-music-raga/asavari28.wav\n/kaggle/input/indian-music-raga/darbari30.wav\n/kaggle/input/indian-music-raga/malkauns03.wav\n/kaggle/input/indian-music-raga/asavari03.wav\n/kaggle/input/indian-music-raga/sarang20.wav\n/kaggle/input/indian-music-raga/sarang02.wav\n/kaggle/input/indian-music-raga/darbari27.wav\n/kaggle/input/indian-music-raga/bageshree28.wav\n/kaggle/input/indian-music-raga/malkauns27.wav\n/kaggle/input/indian-music-raga/bhoopali22.wav\n/kaggle/input/indian-music-raga/malkauns28.wav\n/kaggle/input/indian-music-raga/DKanada05.wav\n/kaggle/input/indian-music-raga/Bhairavi04.wav\n/kaggle/input/indian-music-raga/yaman05.wav\n/kaggle/input/indian-music-raga/yaman23.wav\n/kaggle/input/indian-music-raga/yaman25.wav\n/kaggle/input/indian-music-raga/asavari02.wav\n/kaggle/input/indian-music-raga/bhoop01.wav\n/kaggle/input/indian-music-raga/bhoopali25.wav\n/kaggle/input/indian-music-raga/malkauns02.wav\n/kaggle/input/indian-music-raga/bhairavi28.wav\n/kaggle/input/indian-music-raga/yaman04.wav\n/kaggle/input/indian-music-raga/bageshree31.wav\n/kaggle/input/indian-music-raga/malkauns01.wav\n/kaggle/input/indian-music-raga/bageshree25.wav\n/kaggle/input/indian-music-raga/sarang18.wav\n/kaggle/input/indian-music-raga/asavari01.wav\n/kaggle/input/indian-music-raga/asavari04.wav\n/kaggle/input/indian-music-raga/bhoopali21.wav\n/kaggle/input/indian-music-raga/DKanada04.wav\n/kaggle/input/indian-music-raga/Bhairavi03.wav\n/kaggle/input/indian-music-raga/bageshree24.wav\n/kaggle/input/indian-music-raga/bhairavi31.wav\n/kaggle/input/indian-music-raga/malkauns29.wav\n/kaggle/input/indian-music-raga/bageshree05.wav\n/kaggle/input/indian-music-raga/bageshree01.wav\n/kaggle/input/indian-music-raga/bhoop05.wav\n/kaggle/input/indian-music-raga/bageshree30.wav\n/kaggle/input/indian-music-raga/darbari28.wav\n/kaggle/input/indian-music-raga/asavari05.wav\n/kaggle/input/indian-music-raga/sarang03.wav\n/kaggle/input/indian-music-raga/asavari27.wav\n/kaggle/input/indian-music-raga/malkauns25.wav\n/kaggle/input/indian-music-raga/Bhairavi05.wav\n/kaggle/input/indian-music-raga/sarang01.wav\n/kaggle/input/indian-music-raga/bhoopali23.wav\n/kaggle/input/indian-music-raga/sarang19.wav\n/kaggle/input/indian-music-raga/darbari26.wav\n/kaggle/input/indian-music-raga/Bhairavi02.wav\n/kaggle/input/indian-music-raga/malkauns05.wav\n/kaggle/input/indian-music-raga/bageshree02.wav\n/kaggle/input/indian-music-raga/sarang17.wav\n/kaggle/input/indian-music-raga/bageshree03.wav\n/kaggle/input/indian-music-raga/DKanada02.wav\n/kaggle/input/indian-music-raga/DKanada03.wav\n/kaggle/input/indian-music-raga/yaman26.wav\n/kaggle/input/indian-music-raga/darbari29.wav\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from sklearn.metrics import accuracy_score ,classification_report\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T22:20:14.531298Z",
          "iopub.execute_input": "2024-02-05T22:20:14.531635Z",
          "iopub.status.idle": "2024-02-05T22:20:14.537007Z",
          "shell.execute_reply.started": "2024-02-05T22:20:14.531610Z",
          "shell.execute_reply": "2024-02-05T22:20:14.535645Z"
        },
        "trusted": true,
        "id": "n88lV1TKd2E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, a dataframe consisting of the filenames, labels and the added encoded labels is created"
      ],
      "metadata": {
        "id": "j4l6nmLWd2E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create(root_dir):\n",
        "    file_list = [file for file in os.listdir(root_dir) if file.endswith('.wav')]\n",
        "\n",
        "    metadata = {'filename': [], 'label': []}\n",
        "\n",
        "    for file in file_list:\n",
        "        label = re.search(r'([a-zA-Z]+)', file).group(0)\n",
        "        metadata['filename'].append(file)\n",
        "        metadata['label'].append(label)\n",
        "\n",
        "    df = pd.DataFrame(metadata)\n",
        "    df.to_csv('metadata.csv', index=False)\n",
        "\n",
        "create('/kaggle/input/indian-music-raga')\n",
        "csv_file_path = '/kaggle/working/metadata.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "label_encoder = LabelEncoder()\n",
        "df['encoded_label'] = label_encoder.fit_transform(df['label'])\n",
        "print(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T20:04:20.489054Z",
          "iopub.execute_input": "2024-02-05T20:04:20.489405Z",
          "iopub.status.idle": "2024-02-05T20:04:20.504692Z",
          "shell.execute_reply.started": "2024-02-05T20:04:20.489381Z",
          "shell.execute_reply": "2024-02-05T20:04:20.504089Z"
        },
        "trusted": true,
        "id": "BLTyII4jd2E8",
        "outputId": "4455a84a-250b-4b75-beec-1ef222d7134c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "           filename      label  encoded_label\n0    malkauns04.wav   malkauns              8\n1       yaman02.wav      yaman             10\n2    malkauns26.wav   malkauns              8\n3     asavari25.wav    asavari              2\n4   bageshree04.wav  bageshree              3\n..              ...        ...            ...\n77  bageshree03.wav  bageshree              3\n78    DKanada02.wav    DKanada              1\n79    DKanada03.wav    DKanada              1\n80      yaman26.wav      yaman             10\n81    darbari29.wav    darbari              7\n\n[82 rows x 3 columns]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_updated = '/kaggle/working/updated_metadata.csv'\n",
        "\n",
        "df.to_csv(\"updated_metadata.csv\", index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:37:43.577545Z",
          "iopub.execute_input": "2024-02-05T19:37:43.577867Z",
          "iopub.status.idle": "2024-02-05T19:37:43.582949Z",
          "shell.execute_reply.started": "2024-02-05T19:37:43.577842Z",
          "shell.execute_reply": "2024-02-05T19:37:43.582080Z"
        },
        "trusted": true,
        "id": "GiYcm1mnd2E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, data preprocessing is done. Preprocessing includes cutting, resampling, mixing down, padding and moise reduction as necessary."
      ],
      "metadata": {
        "id": "6skr27Y3d2E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Raga(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 annotations_file,\n",
        "                 audio_dir,\n",
        "                 transformation,\n",
        "                 target_sample_rate,\n",
        "                 num_samples):\n",
        "        self.annotations = pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transformation = transformation\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
        "\n",
        "        label = self.getlabel(index)\n",
        "        signal, sr = torchaudio.load(filename)\n",
        "\n",
        "        if signal is None:\n",
        "            return {\n",
        "                'file': filename,\n",
        "                'audio': None,\n",
        "                'mel': None,\n",
        "                'gt': None,\n",
        "                'duration_seconds': None\n",
        "            }\n",
        "\n",
        "        duration = signal.shape[1] / sr\n",
        "        signal = self.cut(signal)\n",
        "        signal = self.padding(signal)\n",
        "        signal = self.resample(signal, sr)\n",
        "        signal = self.mix_down(signal)\n",
        "        signal=self.noise_reduction(signal)\n",
        "        mel_spec = self.transformation(signal)\n",
        "\n",
        "\n",
        "        sample = {\n",
        "            'file': filename,\n",
        "            'audio': signal,\n",
        "            'mel': mel_spec,\n",
        "            'gt': label,\n",
        "            'duration_seconds': duration\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def cut(self, signal):\n",
        "        if signal.shape[1] > self.num_samples:\n",
        "            signal = signal[:, :self.num_samples]\n",
        "        return signal\n",
        "\n",
        "    def padding(self, signal):\n",
        "        length= signal.shape[1]\n",
        "        if length < self.num_samples:\n",
        "            num_missing_samples = self.num_samples - length\n",
        "            last_dim_padding = (0, num_missing_samples)\n",
        "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "        return signal\n",
        "\n",
        "    def resample(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampled = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampled(signal)\n",
        "        return signal\n",
        "\n",
        "    def mix_down(self, signal):\n",
        "        if signal.shape[0] > 1:\n",
        "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "        return signal\n",
        "\n",
        "    def noise_reduction(self, signal, noise_level=0.002):\n",
        "        # Apply spectral subtraction for noise reduction\n",
        "\n",
        "        stft = torch.stft(signal, n_fft=2048, hop_length=512, window=torch.hann_window(2048),return_complex=True)\n",
        "        magnitude = torch.abs(stft)\n",
        "        phase = torch.angle(stft)\n",
        "\n",
        "        # Estimate noise magnitude\n",
        "        noise_magnitude = torch.mean(magnitude[:, :, :100], dim=2, keepdim=True)\n",
        "\n",
        "        # Apply spectral subtraction\n",
        "        clean_magnitude = torch.max(magnitude - noise_level * noise_magnitude, torch.tensor(0.0))\n",
        "\n",
        "        # Reconstruct the cleaned signal\n",
        "        stft_cleaned = clean_magnitude * torch.exp(1j * phase)\n",
        "        signal_cleaned = torch.istft(stft_cleaned, hop_length=512, window=torch.hann_window(2048), n_fft=2048)\n",
        "\n",
        "        return signal_cleaned\n",
        "\n",
        "    def getlabel(self, index):\n",
        "        if index < len(self.annotations):\n",
        "            label = self.annotations.iloc[index,2]\n",
        "            return label\n",
        "        else:\n",
        "            return None\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T21:49:09.326604Z",
          "iopub.execute_input": "2024-02-05T21:49:09.327417Z",
          "iopub.status.idle": "2024-02-05T21:49:09.341782Z",
          "shell.execute_reply.started": "2024-02-05T21:49:09.327392Z",
          "shell.execute_reply": "2024-02-05T21:49:09.340967Z"
        },
        "trusted": true,
        "id": "2qAUC1GQd2E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Two convolutional layers are used for feature extraction. They apply convolutional operations, ReLU activation functions, max-pooling for downsampling.\n",
        "After convolutional layers, the output is flattened using nn.Flatten() to prepare it for the fully connected layers.\n",
        "A fully connected layer is used for classification.\n",
        "\n",
        "The forward method specifies the forward pass through the network. It involves passing the input through the convolutional layers, flattening the output, applying dropout, and passing through the fully connected layer to obtain the logits.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5wwyN12Vd2E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(10368, 11)\n",
        "        self.dropout= nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        x = self.conv1(input_data)\n",
        "        x = self.conv2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.linear(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T21:07:37.196396Z",
          "iopub.execute_input": "2024-02-05T21:07:37.196704Z",
          "iopub.status.idle": "2024-02-05T21:07:37.203976Z",
          "shell.execute_reply.started": "2024-02-05T21:07:37.196681Z",
          "shell.execute_reply": "2024-02-05T21:07:37.203071Z"
        },
        "trusted": true,
        "id": "Ejywgtb0d2E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions are defined for creating a data loader, training a single epoch, and training the model.\n",
        "The main script checks for GPU availability, creates a dataset object, constructs the model, initializes the loss function and optimizer, and then trains the model using the specified number of epochs. In the main function, the audio data is converted to mel spectograms. Mel spectrograms provide a frequency representation of the audio data which is crucial for raga classification.  They capture the distribution of energy in different frequency bands over time, providing a concise representation of the audio content. Mel spectrograms, by capturing the non-linear characteristics of pitch perception, can enhance the model's ability to discriminate between different pitches and musical nuances."
      ],
      "metadata": {
        "id": "waLQCUwPd2E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "ANNOTATIONS_FILE = '/kaggle/working/updated_metadata.csv'\n",
        "AUDIO_DIR = '/kaggle/input/indian-music-raga'\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "\n",
        "def create_dataloader(train_data, batch_size):\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size, collate_fn=default_collate, shuffle=True)\n",
        "    return train_dataloader\n",
        "\n",
        "\n",
        "def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n",
        "    for batch in data_loader:\n",
        "        input, target = batch['mel'], batch['gt']\n",
        "        input, target = input.to(device), target.to(device)\n",
        "        target = target.long()\n",
        "        prediction = model(input)\n",
        "        loss = loss_fn(prediction, target)\n",
        "\n",
        "        # backpropagate error and update weights\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "    print(f\"loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
        "    for i in range(epochs):\n",
        "        print(f\"Epoch {i+1}\")\n",
        "        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n",
        "        print(\"---------------------------\")\n",
        "    print(\"Finished training\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #checking gpu availability\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    print(f\"Using {device}\")\n",
        "\n",
        "    # instantiating our dataset object and creating data loader\n",
        "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        n_mels=32\n",
        "    )\n",
        "\n",
        "    dataset = Raga(ANNOTATIONS_FILE,\n",
        "                             AUDIO_DIR,\n",
        "                             mel_spectrogram,\n",
        "                             SAMPLE_RATE,\n",
        "                             NUM_SAMPLES\n",
        "                             )\n",
        "\n",
        "    train_dataloader = create_dataloader(dataset, BATCH_SIZE)\n",
        "\n",
        "    # construct model and assign it to device\n",
        "    cnn = CNN().to(device)\n",
        "    print(cnn)\n",
        "\n",
        "    # initialising loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # model training and saving\n",
        "    train(cnn, train_dataloader, loss_fn, optimizer, device, EPOCHS)\n",
        "    torch.save(cnn.state_dict(), \"raga_mel_spec.pth\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T22:21:52.784495Z",
          "iopub.execute_input": "2024-02-05T22:21:52.784814Z",
          "iopub.status.idle": "2024-02-05T22:27:07.034030Z",
          "shell.execute_reply.started": "2024-02-05T22:21:52.784792Z",
          "shell.execute_reply": "2024-02-05T22:27:07.032352Z"
        },
        "trusted": true,
        "id": "sTKbtUd1d2E_",
        "outputId": "4b0f1585-d19c-48f3-8656-201f72cf4957"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Using cpu\nCNN(\n  (conv1): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Dropout(p=0.3, inplace=False)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Dropout(p=0.3, inplace=False)\n  )\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear): Linear(in_features=10368, out_features=14, bias=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n)\nEpoch 1\nloss: 2.3638193607330322\n---------------------------\nEpoch 2\nloss: 2.437779188156128\n---------------------------\nEpoch 3\nloss: 1.6375066041946411\n---------------------------\nEpoch 4\nloss: 18.65873146057129\n---------------------------\nEpoch 5\nloss: 2.1578731536865234\n---------------------------\nEpoch 6\nloss: 9.156569480895996\n---------------------------\nEpoch 7\nloss: 1.4172712564468384\n---------------------------\nEpoch 8\nloss: 0.0\n---------------------------\nEpoch 9\nloss: 0.09531626850366592\n---------------------------\nEpoch 10\nloss: 0.2532215714454651\n---------------------------\nEpoch 11\nloss: 0.7875905632972717\n---------------------------\nEpoch 12\nloss: 3.267106294631958\n---------------------------\nEpoch 13\nloss: 0.26467275619506836\n---------------------------\nEpoch 14\nloss: 0.0\n---------------------------\nEpoch 15\nloss: 3.242814540863037\n---------------------------\nEpoch 16\nloss: 0.3576643168926239\n---------------------------\nEpoch 17\nloss: 0.024950280785560608\n---------------------------\nEpoch 18\nloss: 0.0\n---------------------------\nEpoch 19\nloss: 0.3458079695701599\n---------------------------\nEpoch 20\nloss: 0.008318892680108547\n---------------------------\nEpoch 21\nloss: 1.525060772895813\n---------------------------\nEpoch 22\nloss: 0.0003181189822498709\n---------------------------\nEpoch 23\nloss: 0.07054996490478516\n---------------------------\nEpoch 24\nloss: 0.08391818404197693\n---------------------------\nEpoch 25\nloss: 0.2143326699733734\n---------------------------\nEpoch 26\nloss: 0.00497662415727973\n---------------------------\nEpoch 27\nloss: 2.0861407392658293e-05\n---------------------------\nEpoch 28\nloss: 0.0029103087726980448\n---------------------------\nEpoch 29\nloss: 2.017542839050293\n---------------------------\nEpoch 30\nloss: 8.344646857949556e-07\n---------------------------\nFinished training\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "ANNOTATIONS_FILE = '/kaggle/working/updated_metadata.csv'\n",
        "AUDIO_DIR =  '/kaggle/input/indian-music-raga'\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "\n",
        "def predict(model, input, target):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input)\n",
        "\n",
        "        predicted_index = predictions.argmax(dim=1).item()\n",
        "        expected_index = target\n",
        "    return expected_index ,predicted_index\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T22:27:38.119030Z",
          "iopub.execute_input": "2024-02-05T22:27:38.119336Z",
          "iopub.status.idle": "2024-02-05T22:27:38.125159Z",
          "shell.execute_reply.started": "2024-02-05T22:27:38.119316Z",
          "shell.execute_reply": "2024-02-05T22:27:38.124249Z"
        },
        "trusted": true,
        "id": "yWCzlqNpd2E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we evaluate the model's predictions and prints its accuracy"
      ],
      "metadata": {
        "id": "S0LJ25TWd2E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn = CNN()\n",
        "state_dict = torch.load(\"raga_mel_spec.pth\")\n",
        "cnn.load_state_dict(state_dict)\n",
        "\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    n_fft=400,\n",
        "    hop_length=160,\n",
        "    n_mels=32\n",
        ")\n",
        "predicted_labels=[]\n",
        "expected_labels=[]\n",
        "\n",
        "testing = Raga(ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mel_spectrogram,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES)\n",
        "\n",
        "\n",
        "\n",
        "random_indices_list = torch.randint(0, len(testing), (81,)).tolist()\n",
        "for index in random_indices_list:\n",
        "    data_point = testing[index]\n",
        "    input, target = data_point['mel'], data_point['gt']\n",
        "    input.unsqueeze_(0)\n",
        "    expected_index ,predicted_index = predict(cnn, input, target)\n",
        "    predicted_labels.append(predicted_index)\n",
        "    expected_labels.append(expected_index)\n",
        "\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(expected_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(expected_labels, predicted_labels))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "df['predicted'] = predicted_labels\n",
        "df['expected'] = expected_labels\n",
        "\n",
        "print(df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T22:27:57.679200Z",
          "iopub.execute_input": "2024-02-05T22:27:57.679529Z",
          "iopub.status.idle": "2024-02-05T22:28:07.269223Z",
          "shell.execute_reply.started": "2024-02-05T22:27:57.679506Z",
          "shell.execute_reply": "2024-02-05T22:28:07.268241Z"
        },
        "trusted": true,
        "id": "MA6h0i9ed2FA",
        "outputId": "cb62a63b-3736-48f1-bb08-13b1e9d3647a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.8888888888888888\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         2\n           1       1.00      1.00      1.00         3\n           2       0.65      1.00      0.79        11\n           3       0.87      0.93      0.90        14\n           4       1.00      1.00      1.00         6\n           5       1.00      0.67      0.80         9\n           6       1.00      1.00      1.00         1\n           7       1.00      1.00      1.00         6\n           8       1.00      0.67      0.80         9\n           9       1.00      0.75      0.86         8\n          10       0.92      1.00      0.96        12\n\n    accuracy                           0.89        81\n   macro avg       0.95      0.91      0.92        81\nweighted avg       0.92      0.89      0.89        81\n\n    predicted  expected\n0           4         4\n1           3         9\n2           3         3\n3           6         6\n4           9         9\n..        ...       ...\n76          0         0\n77          2         2\n78          4         4\n79         10        10\n80          2         2\n\n[81 rows x 2 columns]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get an accuracy of 88.89% as depicted above"
      ],
      "metadata": {
        "id": "Gt7iN_g4d2FA"
      }
    }
  ]
}
